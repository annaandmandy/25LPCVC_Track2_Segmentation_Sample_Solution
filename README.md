# Sample Solution - Track 2: Open-Vocabulary Segmentation with Text-Prompt (LPCVC 2025)

## :fire: News
- [2025.02.13] OpenCV Webinar by Professor Lu introducing LPCVC 2025
- [2025.02.01] Sample solution of Track2: X-Decoder released
- [2025.01.10] LPCVC 2025 accepted as CVPR 2025 Workshop
- [2024.12.10] LPCVC 2025 announced at NeurIPS 2024

## 1. Model Training and Evaluation
:point_right: ***Please refer to [[XDecoder]](https://github.com/microsoft/X-Decoder) for model training and evaluation details.***

### Environment Setup
```sh
# create conda virtual env
conda create -n xdecoder python=3.11 anaconda
conda activate xdecoder
conda install pip
# install required packages (XDecoder), [NO Mask2Former needed as there is no deformableAttention in the baseline model. ignored libs: mpi4py # difficult to install due to conflicts of versions ]
pip install -r ./requirements.txt
# isntall custom tools
pip install git+https://github.com/arogozhnikov/einops.git
pip install git+https://github.com/openai/whisper.git # audio to text, [NOT needed for LPCVC 2025 Track2]
pip install git+https://github.com/facebookresearch/detectron2
pip install git+https://github.com/cocodataset/panopticapi.git # for coco dataset preparation
```

### Dataset Preparation
```sh
# create the xdecoder_data folder
cd /PATH/TO/DATASET
mkdir .xdecoder_data
cd .xdecoder_data
mkdir coco
cd coco
# Prepare panoptic_train2017, panoptic_semseg_train2017 exactly the same as [Mask2Fomer](https://github.com/facebookresearch/Mask2Former/tree/main/datasets)

# download coco 2017 dataset images: https://cocodataset.org/#download
wget http://images.cocodataset.org/zips/train2017.zip
unzip train2017.zip
wget http://images.cocodataset.org/zips/val2017.zip
unzip val2017.zip
wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip
unzip annotations_trainval2017.zip # extract instance train/val json files to ./annotations
wget http://images.cocodataset.org/annotations/panoptic_annotations_trainval2017.zip
unzip panoptic_annotations_trainval2017.zip # extract panoptic train/val json files to ./annotations 

# (SEEM & X-Decoder) Download additional logistic and custom annotation files to .xdecoder_data/coco/annotations
wget https://huggingface.co/xdecoder/X-Decoder/resolve/main/caption_class_similarity.pth -P ./annotations/
wget https://huggingface.co/xdecoder/X-Decoder/resolve/main/captions_train2017_filtrefgumdval_filtvlp.json -P ./annotations/
wget https://huggingface.co/xdecoder/X-Decoder/resolve/main/grounding_train2017_filtrefgumdval_filtvlp.json -P ./annotations/
wget https://huggingface.co/xdecoder/X-Decoder/resolve/main/panoptic_train2017_filtrefgumdval_filtvlp.json -P ./annotations/
wget https://huggingface.co/xdecoder/X-Decoder/resolve/main/refcocog_umd_val.json
wget https://github.com/peteanderson80/coco-caption/blob/master/annotations/captions_val2014.json -P ./annotations/

# (SEEM) Download LVIS annotations for mask preparation
wget https://huggingface.co/xdecoder/SEEM/resolve/main/coco_train2017_filtrefgumdval_lvis.json -P ./annotations/

# prepare coco panotpic annotation by function from Mask2Former [https://github.com/facebookresearch/Mask2Former/blob/main/datasets/prepare_coco_semantic_annos_from_panoptic_annos.py]
export DETECTRON2_DATASETS=./xdecoder_dataset # /path/to/datasets
python prepare_coco_semantic_annos_from_panoptic_annos.py 
# help ref: [OneFormer](https://github.com/SHI-Labs/OneFormer/blob/main/datasets/README.md)


```


After dataset preparation, the dataset structure would be:
```
.xdecoder_data
└── coco/
    ├── train2017/ # images #(?)
    ├── val2017/ # images  #(?)
    ├── panoptic_train2017/ # png annotations #(?)
    ├── panoptic_semseg_train2017/ # generated by the script following Mask2Former as mentioned above
    ├── panoptic_val2017/ # png annotations #(?)
    ├── panoptic_semseg_val2017/ # generated by the script following Mask2Former as mentioned above
    └── annotations/
        ├── refcocog_umd_val.json
        ├── captions_val2014.json
        ├── caption_class_similarity.pth
        ├── panoptic_train2017_filtrefgumdval_filtvlp.json
        └── grounding_train2017_filtrefgumdval_filtvlp.json
        └── instances_{train,val}2017.json
        └── panoptic_{train,val}2017.json
└── lvis/
    └── coco_train2017_filtrefgumdval_lvis.json
```

#### Evaluation Dataset - RefCOCO (SEEM & X-Decoder)
Please refer to COCO Preparation on [line](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/blob/v1.0/assets/readmes/DATASET.md#coco-seem--x-decoder).

**You can set the location for builtin datasets by `export DETECTRON2_DATASETS=/path/to/datasets`. If left unset, the default is ./datasets relative to your current working directory.**

### Model Details
- Architecture: Focal-T / ViT-b
- Training data: COCO
- Evaluation data: RefCOCOg
- Task: Grounding Segmentation

### Getting Started
- Training command: `sh command.sh`
- Pre-trained weights: [[Google Drive]](https://drive.google.com/file/d/1pk1HVDvQuGEyGwB4fP6y35mLWqY5xqOq/view?usp=drive_link)
  - Download to: `./lpcvc_track2_models/model_state_dict.pt`

### :bulb: Important Tips
- Higher input resolution generally improves segmentation accuracy but increases computational cost. Consider this trade-off carefully.
- Some complex operations (e.g., GroupNorm, DeformableAttention) may not be well optimized/supported by QNN libraries. Consider using alternative implementations. Check QNN documentation for supported operations.

## 2. Model Compilation and Profiling (Qualcomm AI Hub)
For detailed instructions on model compilation, profiling and inference, refer to the [[AI Hub Documentation]](https://app.aihub.qualcomm.com/docs/).

:point_right: ***LPCVC 2025 Track 2, Sample solution compilation, profiling and inference pipeline available in [[compile_profile_inference_aihub]](./compile_and_profile)***. Run `python ./compile_and_profile/compile_profile_inference_aihub.py` to get your torch model inference, converting to onnx format, andn QNN model **compiling + profiling + inference** on AIHub all together.

```python
# Submit compilation job to AIHub
compile_job = hub.submit_compile_job(
    model=model_path,
    name="lpcvc25_track2_sample_solution",
    device=hub.Device(deploy_device),
    options="--target_runtime qnn_context_binary",
)
# IMPORTANT! You must share your compile job to lpcvc organizers thus we can pull and evalaute it.
compile_job.modify_sharing(add_emails=['lowpowervision@gmail.com'])
model = compile_job.get_target_model()

# Profile model if requested
profile_job = hub.submit_profile_job(
    name="lpcvc25_track2_sample_solution",
    model=model, 
    device=hub.Device(deploy_device)
)
```

## 3. Inference and Evaluation

:point_right: ***See [[compile_profile_inference_aihub]](./compile_and_profile) for complete inference and evaluation pipeline.***

#### :warning: Important Note
During evaluation, only the following inference commands will be used. Ensure your submitted model is correctly compiled and produces valid outputs on AIHub:

```python
# Prepare inputs
aihub_inputs = {
    'image_input': [image_input.detach().cpu().numpy()], 
    'text_input': [text_input.detach().cpu().numpy()]
}

# Run inference
inference_job = hub.submit_inference_job(
    name="lpcvc25_track2_sample_solution",
    model=model,
    device=hub.Device(deploy_device),
    inputs=aihub_inputs
)
qnn_outputs = inference_job.download_output_data() # shape=[1024, 1024], numpy.array
```

### Evaluation Details

#### Device
- Snapdragon X Elite CRD

#### Test Dataset
- 1000 images from ~200 categories
- 3-5 annotated masks per image
- Balanced across mask sizes and categories
- 3-5 text descriptions per mask
- Text descriptions include:
  - Keywords
  - Short phrases
  - Detailed sentences (appearance, location, semantics, relationships, etc.)

#### Input Format
- **Image**:
  - RGB format, shape: 3x1024x1024
  - Longest edge resized to 1024, padded to square
- **Text**:
  - Shape: 2x1x77 (tokenized embedding + attention mask)
  - Uses CLIP tokenizer output

```python
# Image preprocessing example
img = Image.open(image_path).convert("RGB")
transform = transforms.Compose([
    transforms.Resize(1000, max_size=1024),
])
image = transform(img)
image = torch.from_numpy(np.asanyarray(image)).float().permute(2, 0, 1)
images = [image]
image_input = ImageList.from_tensors(images, size_divisibility=1024).tensor

# All the input images have the same input shape 3x1024x1024 with RGB values [0, 255]. The original images are first resized to make the longest edge equals 1024, then padded to square 1024x1024 by 0s.
```

#### Text Processing
- Uses OpenAI CLIP tokenizer
- Format: `[input_ids; attention_mask]`

```python
# Text tokenization
tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')
tokenizer.add_special_tokens({'cls_token': tokenizer.eos_token})
tokens = tokenizer(text, padding='max_length', truncation=True, max_length=77, return_tensors='pt')
text_input = torch.stack((tokens['input_ids'], tokens['attention_mask']))  # Shape: 2x1x77

# (Text tokenization) QNN library does not support tokenization of text input yet. In order to reduce the influence of different text tokenizer used to the final performance, accuracy and latency, we pre-fixed the text tokenizer and only input the tokenized vector of the input text to the model
```

#### Evaluation Metric
- **mIoU** (mean Intersection over Union)

```python
def compute_IoU(pred_seg, gd_seg):
    I = (pred_seg & gd_seg)
    U = (pred_seg | gd_seg)
    return I.sum() / (U.sum() + 1e-6)

# Compute mIoU across test set
pred = output['grounding_mask']  # Binary mask (threshold prediction.sigmoid() > 0.5)
gt = input['groundings']['masks'].bool()
IoUs = [compute_IoU(p, g) for p, g in zip(pred, gt)]
mIoU = sum(IoUs) / len(IoUs) * 100
```



## Acknowledgements
* The sample solution for LPCVC 2025 Track-2 is built on [XDecoder](https://github.com/microsoft/X-Decoder)

## Contact
LPCVC 2025 Organizers:
- Website: [Homepage](https://lpcv.ai/)
- Community: [Slack](https://aihub.qualcomm.com/community/slack)
- Email: [lowpowervision@gmail.com](mailto:lowpowervision@gmail.com)
