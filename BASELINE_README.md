
## Baseline Intruction
:point_right: ***Please refer to [[XDecoder]](https://github.com/microsoft/X-Decoder) for model training and evaluation details.***

### 1. Environment Setup
```sh
# create conda virtual env
conda create -n xdecoder python=3.11 anaconda
conda activate xdecoder
conda install pip
# install required packages (XDecoder), [NO Mask2Former needed as there is no deformableAttention in the baseline model. ignored libs: mpi4py # difficult to install due to conflicts of versions ]
pip install -r ./requirements.txt
# isntall custom tools
pip install git+https://github.com/arogozhnikov/einops.git
pip install git+https://github.com/openai/whisper.git # audio to text, [NOT needed for LPCVC 2025 Track2]
pip install git+https://github.com/facebookresearch/detectron2
pip install git+https://github.com/cocodataset/panopticapi.git # for coco dataset preparation
```

### 2. Dataset Preparation
```sh
# create the xdecoder_data folder
cd /PATH/TO/DATASET
mkdir .xdecoder_data
cd .xdecoder_data
mkdir coco
cd coco
# Prepare panoptic_train2017, panoptic_semseg_train2017 exactly the same as [Mask2Fomer](https://github.com/facebookresearch/Mask2Former/tree/main/datasets)

# download coco 2017 dataset images: https://cocodataset.org/#download
wget http://images.cocodataset.org/zips/train2017.zip
unzip train2017.zip
wget http://images.cocodataset.org/zips/val2017.zip
unzip val2017.zip
wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip
unzip annotations_trainval2017.zip # extract instance train/val json files to ./annotations
wget http://images.cocodataset.org/annotations/panoptic_annotations_trainval2017.zip
unzip panoptic_annotations_trainval2017.zip # extract panoptic train/val json files to ./annotations 
unzip ./annotations/panoptic_train2017.zip -d ./ # extract panoptic train/val png annotations to ./panoptic_train2017
unzip ./annotations/panoptic_val2017.zip -d ./ # extract panoptic train/val png annotations to ./panoptic_val2017



# (SEEM & X-Decoder) Download additional logistic and custom annotation files to .xdecoder_data/coco/annotations
wget https://huggingface.co/xdecoder/X-Decoder/resolve/main/caption_class_similarity.pth -P ./annotations/
wget https://huggingface.co/xdecoder/X-Decoder/resolve/main/captions_train2017_filtrefgumdval_filtvlp.json -P ./annotations/
wget https://huggingface.co/xdecoder/X-Decoder/resolve/main/grounding_train2017_filtrefgumdval_filtvlp.json -P ./annotations/
wget https://huggingface.co/xdecoder/X-Decoder/resolve/main/panoptic_train2017_filtrefgumdval_filtvlp.json -P ./annotations/
wget https://huggingface.co/xdecoder/X-Decoder/resolve/main/refcocog_umd_val.json
wget https://github.com/peteanderson80/coco-caption/blob/master/annotations/captions_val2014.json -P ./annotations/

# (SEEM) Download LVIS annotations for mask preparation
wget https://huggingface.co/xdecoder/SEEM/resolve/main/coco_train2017_filtrefgumdval_lvis.json -P ./annotations/

# prepare coco panotpic annotation by function from Mask2Former [https://github.com/facebookresearch/Mask2Former/blob/main/datasets/prepare_coco_semantic_annos_from_panoptic_annos.py]
export DETECTRON2_DATASETS=/home/scott/Work/Work/Qualcomm/LPCVC_2025_Track2_OVSeg/xdecoder_data # /path/to/datasets/root
# **You can set the location for builtin datasets by `export DETECTRON2_DATASETS=/path/to/datasets`. If left unset, the default is ./datasets relative to your current working directory.**


python prepare_coco_semantic_annos_from_panoptic_annos.py 
# help ref: [OneFormer](https://github.com/SHI-Labs/OneFormer/blob/main/datasets/README.md)
```


After dataset preparation, the dataset structure would be:
```
.xdecoder_data
└── coco/
    ├── train2017/ # images
    ├── val2017/ # images
    ├── panoptic_train2017/ # png annotations
    ├── panoptic_semseg_train2017/ # generated by the script following Mask2Former as mentioned above
    ├── panoptic_val2017/ # png annotations
    ├── panoptic_semseg_val2017/ # generated by the script following Mask2Former as mentioned above
    └── annotations/
        ├── refcocog_umd_val.json # used for ov-seg evaluation
        ├── captions_val2014.json
        ├── caption_class_similarity.pth
        ├── panoptic_train2017_filtrefgumdval_filtvlp.json
        └── grounding_train2017_filtrefgumdval_filtvlp.json # used for ov-seg training
        └── instances_{train,val}2017.json
        └── panoptic_{train,val}2017.json
└── lvis/
    └── coco_train2017_filtrefgumdval_lvis.json
```

#### Evaluation Dataset - RefCOCO (SEEM & X-Decoder)
During training, refcoco ('refcocog_umd_val.json') is a good dataset used to evaluate the model for ov-segment with text-prompt (following X-Decoder)


### 3. Baseline Model Details
- Architecture: Focal-T / ViT-b
- Training data: COCO
- Evaluation data: RefCOCOg
- Task: OV-Seg with text-prompt (Grounding)
- Download pre-trained weights
```sh
xdecoder
```

### 4. Getting Started
- Training command: `sh command.sh`
- Pre-trained weights: [[Google Drive]](https://drive.google.com/file/d/1pk1HVDvQuGEyGwB4fP6y35mLWqY5xqOq/view?usp=drive_link)
  - Download to: `./lpcvc_track2_models/model_state_dict.pt`

### :bulb: Tips
- Higher input resolution generally improves segmentation accuracy but increases computational cost. Consider this trade-off carefully.
- Some complex operations (e.g., GroupNorm, DeformableAttention) may not be well optimized/supported by QNN libraries. Consider using alternative implementations. Check QNN documentation for supported operations.
